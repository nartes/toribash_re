\documentclass[oneside, final, 12pt]{amsart}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[paper=a4paper, left=3cm, top=2cm, bottom=2cm, right=1cm]{geometry}
\usepackage{indentfirst}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{multicol}
\usepackage{algorithmic}
\usepackage{moreverb}
\usepackage{listings}

% listings configuration
\lstdefinestyle{mystyle}{
  numbers=left,
  numbersep=5pt,
  tabsize=2
}

\lstset{style=mystyle}

\begin{document}

\begin{titlepage}

%\newgeometry{margin=1cm}

\centerline{\bf Siarhei Siniak}
\bigskip
\bigskip
\center{\large \bf Reinforcement learning with Toribash}
\vfill
\centerline{\large \bf Minsk, Belarus, 2018}

\restoregeometry

\end{titlepage}

\setcounter{page}{2}

\tableofcontents

\cleardoublepage

\section{Introduction}

There's a game.
It consists of $\mathcal{S}$-states, $\mathcal{R}$-rewards, $\mathcal{A}$-actions.
The goal is to apply \textbf{DDPG} like architecture and make an agent
that maximizes the expected reward over episodes.

Consider a time $i$. There's an injury $\left\{r_i^p\right\}_{p=0}^1$,
where $p$ corresponds to a player number.
$s_{i-k}, s_{i-k+1}, \dots, s_{i-1}$ are game states that are
for a injury prediction at a time step $i$. \\
$a_{i-k},a_{i-k+1},\dots, a_{i-1}$ are actions taken before a transition
to next steps $s_{i-k+1}, s_{i-k+2}, \dots, s_i$ correspondingly.
The state $s_i$ is not available to a model as well as action $a_i$.

We are considering just a critic part of \textbf{DDPG} and it is to predict
an instantaneous reward. No need for cumulative reward prediction and no notion
of Bellman's equation will be taken for a while.
This model is a part of decision making process, i.e. it will have an impact
on what action to choose.

Define $\pi_{\Theta}(s)$ exploration policy. The goal is to understand the game's nature.
Maximization of expected reward is not required.
Let's set $\pi_{\Theta}(s)$ as a uniform distribution over available actions.
In our case these are 20 controls with 4 states and yet 2 controls with 2 states.
Each opponent has 22 controls in total. Each game episode consists of 46 steps.

Up to now there were mined $\approx 2.5$ millions of steps from Toribash with
uniform action policy for both opponents.

Our model takes $k$ previous steps and $k$ actions chosed at those moments and
classifies whether an injury $r_i$ is positive or equals zero.
If this model works well, we shift to a regression of injury based on states
and actions. And when later succeeds it makes sense to consider a whole actor-critic
model from \textbf{DDPG} architecture.

The dataset is imbalanced. The distribution of injury classes is $87\%$ for 0s
and $13\%$ for 1s.
Since both classes are of equal importance, we are going to use weighted sampling of data.
$50\%$ of 1s and $50\%$ of 0s are to be among sampled batches.

A straight forward CNN architecture gets $88.5\%$ of accuracy, with $90\%$ recall for 1s,
and $87\%$ for 0s respectively.

To use recurrent model requires balanced sampling of contiguous sequences of states.
Define $m+1$ as a length of a sequence. We are to classify $\left(r_i > 0\right)_{i=k}^m$
using $s_{i-k}, \dots, s_{i-1}; a_{i-k}, \dots, a_{i-1}$.
So effectively a sliding window of length $k$ is moved toward states
with bigger indexes.
it simulates a part of game episode.

\cleardoublepage

\section{Conclusion}
\textbf{TODO:} add something here ...

\cleardoublepage

\addcontentsline{toc}{section}{\bibname}
\begin{thebibliography}{0}

  \bibitem{ddpg-lh-1509} Timothy P. Lillicrap, Jonathan J. Hunt Continuous control with deep
    reinforcement learning, 2015

  \bibitem{L:toribash-github} https://github.com/nartes/toribash\_re

\end{thebibliography}

\end{document}
% vi: tabstop=2 sw=2 sts=2
